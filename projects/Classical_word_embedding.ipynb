{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Example text\n",
    "sentences = [\n",
    "    \"I love natural language processing\",\n",
    "    \"Word2Vec creates word embeddings\",\n",
    "    \"Deep learning is amazing for NLP\",\n",
    "    \"black and brown are colors\"\n",
    "]\n",
    "\n",
    "# Tokenize (each sentence becomes a list of words)\n",
    "tokenized = [word_tokenize(sent.lower()) for sent in sentences]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(\n",
    "    sentences=tokenized,\n",
    "    vector_size=100,   # embedding dimension\n",
    "    window=5,          # context window size\n",
    "    min_count=1,       # ignore words with low frequency\n",
    "    workers=4,         # parallel training\n",
    "    sg=1               # 1 for skip-gram, 0 for CBOW\n",
    ")\n",
    "\n",
    "# Save model\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "# Load later\n",
    "model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00719094  0.00423289  0.00216339  0.00744071 -0.00488927 -0.00456435\n",
      " -0.00609817  0.00329937 -0.00449946  0.00852288]\n",
      "[('are', 0.12300866842269897), ('creates', 0.08058694750070572), ('black', 0.06548557430505753), ('colors', 0.054333679378032684), ('language', 0.01606523059308529), ('learning', 0.013243247754871845), ('embeddings', 0.010695216245949268), ('i', 0.0007033531437627971), ('is', -0.0037013525143265724), ('word', -0.014541246928274632)]\n",
      "0.016065232\n",
      "0.05959967\n"
     ]
    }
   ],
   "source": [
    "# Get vector for a word\n",
    "vector = model.wv[\"nlp\"]\n",
    "print(vector[:10])  # first 10 dims\n",
    "\n",
    "# Find most similar words\n",
    "print(model.wv.most_similar(\"nlp\"))\n",
    "\n",
    "# Check similarity between words\n",
    "print(model.wv.similarity(\"nlp\", \"language\"))\n",
    "print(model.wv.similarity(\"brown\", \"black\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('kings', 0.7138044834136963), ('queen', 0.6510957479476929), ('monarch', 0.6413194537162781), ('crown_prince', 0.6204219460487366), ('prince', 0.6159994602203369), ('sultan', 0.5864822864532471), ('ruler', 0.5797566175460815), ('princes', 0.5646552443504333), ('Prince_Paras', 0.5432944297790527), ('throne', 0.5422106385231018)]\n",
      "[('universities', 0.7003917694091797), ('faculty', 0.6780906319618225), ('unversity', 0.6758289337158203), ('undergraduate', 0.6587095260620117), ('univeristy', 0.6585440039634705), ('campus', 0.6434986591339111), ('college', 0.6385268568992615), ('academic', 0.6317197680473328), ('professors', 0.6298648715019226), ('undergraduates', 0.6149813532829285)]\n",
      "0.6780907\n",
      "[[0.6780907]]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# Example\n",
    "print(model.most_similar(\"king\"))\n",
    "print(model.most_similar(\"university\"))\n",
    "print(model.similarity(\"university\", \"faculty\"))\n",
    "print(cosine_similarity(model[\"university\"].reshape(1, -1), model[\"faculty\"].reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'wget' is not recognized as an internal or external command,\",\n",
       " 'operable program or batch file.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!wget https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in dictionary = 6\n",
      "Dictionary is = {'text': 1, 'the': 2, 'leader': 3, 'prime': 4, 'natural': 5, 'language': 6}\n",
      "Dense vector for word with index 1 => [-0.81660002 -0.27568999 -0.32951    -0.19751    -0.13119     0.58964998\n",
      " -0.47442999 -0.20592999  0.32954001 -1.58150005  0.22524001  0.19357\n",
      "  0.26813     0.18815     0.20840999 -0.2043     -0.25769001 -0.0095931\n",
      "  0.020479   -0.4172     -0.28132999 -0.3998     -0.16343001  0.59351999\n",
      " -0.053312   -0.073362    0.39872    -0.053993    0.62893999 -0.040527\n",
      "  0.20638999  0.31568    -0.62093002  0.74989003 -0.79408002 -0.30658001\n",
      "  0.25953001 -0.74764001 -0.25007999  0.28804001 -0.19340999 -0.046537\n",
      " -0.47253999  0.080808    0.045168    0.3531     -0.15433     0.26328999\n",
      " -0.66061997 -0.63936001]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "texts = ['text', 'the', 'leader', 'prime', 'natural', 'language']\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "print(\"Number of unique words in dictionary =\", len(tokenizer.word_index))\n",
    "print(\"Dictionary is =\", tokenizer.word_index)\n",
    "\n",
    "def embedding_for_vocab(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # +1 for padding token (index 0)\n",
    "    embedding_matrix_vocab = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word]\n",
    "                embedding_matrix_vocab[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix_vocab\n",
    "\n",
    "embedding_dim = 50 # match this with glove file\n",
    "glove_path = \".\\glove-6B-300d.txt\"\n",
    "embedding_matrix_vocab = embedding_for_vocab(glove_path, tokenizer.word_index, embedding_dim)\n",
    "\n",
    "first_word_index = 1  # Tokenizer indexes start from 1\n",
    "print(\"Dense vector for word with index 1 =>\", embedding_matrix_vocab[first_word_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words loaded: 400000\n",
      "Vector for 'king': [ 0.0033901 -0.34614    0.28144    0.48382    0.59469    0.012965\n",
      "  0.53982    0.48233    0.21463   -1.0249   ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load GloVe file\n",
    "def load_glove_embeddings(path):\n",
    "    embeddings_index = {}\n",
    "    with open(path, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = vector\n",
    "    return embeddings_index\n",
    "\n",
    "embeddings_index = load_glove_embeddings(glove_path)\n",
    "\n",
    "print(\"Words loaded:\", len(embeddings_index))\n",
    "print(\"Vector for 'king':\", embeddings_index[\"king\"][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity(king, queen): 0.63364697\n",
      "Similarity(dog, cat): 0.6816747\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def similarity(word1, word2):\n",
    "    v1 = embeddings_index[word1].reshape(1, -1)\n",
    "    v2 = embeddings_index[word2].reshape(1, -1)\n",
    "    return cosine_similarity(v1, v2)[0][0]\n",
    "\n",
    "print(\"Similarity(king, queen):\", similarity(\"king\", \"queen\"))\n",
    "print(\"Similarity(dog, cat):\", similarity(\"dog\", \"cat\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
